#!/usr/bin/env python3
"""
æ¨¡å‹æ•ˆæœæµ‹è¯•è„šæœ¬
è¯„ä¼°å‡†ç¡®ç‡ã€å¬å›ç‡ã€F1ã€Top-1/Top-5ã€æ¨ç†é€Ÿåº¦
"""

import argparse
import base64
import json
import time
import statistics
from pathlib import Path
from collections import defaultdict
from typing import Dict, List, Tuple
import requests

# Import ICAO code mapping
from icao_to_fullname_mapping import get_fullname


class AccuracyTester:
    """æ¨¡å‹æ•ˆæœæµ‹è¯•å™¨"""

    def __init__(self, base_url: str, data_dir: str, output_file: str = None):
        self.base_url = base_url.rstrip('/')
        self.data_dir = Path(data_dir)
        self.output_file = output_file

        # ç»Ÿè®¡æ•°æ®
        self.results = []
        self.latencies = []

    def parse_ground_truth(self, filename: str) -> str:
        """ä»æ–‡ä»¶åè§£æçœŸå®æ ‡ç­¾"""
        # å‡è®¾æ–‡ä»¶åæ ¼å¼ä¸º: LABEL-0001.jpg
        # ä¾‹å¦‚: A320-0001.jpg
        parts = filename.replace('.jpg', '').split('-')
        if parts:
            return parts[0]
        return filename

    def load_test_images(self) -> List[Tuple[Path, str]]:
        """åŠ è½½æµ‹è¯•å›¾ç‰‡å’ŒçœŸå®æ ‡ç­¾"""
        images = []
        for img_file in self.data_dir.glob('*.jpg'):
            ground_truth = self.parse_ground_truth(img_file.name)
            images.append((img_file, ground_truth))
        return images

    def test_single_image(self, image_path: Path, ground_truth: str) -> Dict:
        """æµ‹è¯•å•å¼ å›¾ç‰‡"""
        # åŠ è½½å›¾ç‰‡
        with open(image_path, 'rb') as f:
            img_data = f.read()
        base64_img = base64.b64encode(img_data).decode()

        # å°† ICAO ä»£ç è½¬æ¢ä¸ºå®Œæ•´åç§°ä»¥åŒ¹é…æ¨¡å‹è¾“å‡º
        expected_fullname = get_fullname(ground_truth)

        # å‘é€APIè¯·æ±‚
        start_time = time.time()
        try:
            response = requests.post(
                f"{self.base_url}/api/v1/aircraft",
                json={'image': base64_img},
                timeout=60
            )
            duration = (time.time() - start_time) * 1000  # æ¯«ç§’

            if response.status_code == 200:
                result = response.json()
                top1_pred = result['top1']['class']
                top1_conf = result['top1']['confidence']
                predictions = [p['class'] for p in result['predictions'][:5]]

                return {
                    'success': True,
                    'ground_truth': ground_truth,
                    'expected_fullname': expected_fullname,
                    'top1_prediction': top1_pred,
                    'top1_confidence': top1_conf,
                    'top5_predictions': predictions,
                    'top1_correct': top1_pred == expected_fullname,
                    'top5_correct': expected_fullname in predictions,
                    'latency_ms': duration,
                    'image': image_path.name
                }
            else:
                return {
                    'success': False,
                    'error': f"HTTP {response.status}",
                    'ground_truth': ground_truth,
                    'latency_ms': duration,
                    'image': image_path.name
                }

        except Exception as e:
            duration = (time.time() - start_time) * 1000
            return {
                'success': False,
                'error': str(e),
                'ground_truth': ground_truth,
                'latency_ms': duration,
                'image': image_path.name
            }

    def run(self):
        """è¿è¡Œå®Œæ•´çš„æµ‹è¯•"""
        print(f"ğŸ§ª å¼€å§‹æ¨¡å‹æ•ˆæœæµ‹è¯•...")
        print(f"   åŸºç¡€URL: {self.base_url}")
        print(f"   æ•°æ®ç›®å½•: {self.data_dir}")
        print()

        # åŠ è½½æµ‹è¯•å›¾ç‰‡
        test_images = self.load_test_images()
        print(f"   æ‰¾åˆ° {len(test_images)} å¼ æµ‹è¯•å›¾ç‰‡")
        print()

        # æµ‹è¯•æ¯å¼ å›¾ç‰‡
        for i, (image_path, ground_truth) in enumerate(test_images, 1):
            print(f"   æµ‹è¯• {i}/{len(test_images)}: {image_path.name}")
            result = self.test_single_image(image_path, ground_truth)
            self.results.append(result)

            if result['success']:
                self.latencies.append(result['latency_ms'])

        # è®¡ç®—æŒ‡æ ‡
        metrics = self.calculate_metrics()
        self.print_results(metrics)

        # ä¿å­˜ç»“æœ
        if self.output_file:
            self.save_results(metrics)

        return metrics

    def calculate_metrics(self) -> Dict:
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        successful_results = [r for r in self.results if r['success']]
        total_count = len(self.results)
        success_count = len(successful_results)

        if success_count == 0:
            return {
                'total_images': total_count,
                'successful_tests': success_count,
                'failed_tests': total_count - success_count,
                'top1_accuracy': 0.0,
                'top5_accuracy': 0.0,
                'avg_latency_ms': 0.0,
                'p50_latency_ms': 0.0,
                'p95_latency_ms': 0.0,
                'p99_latency_ms': 0.0
            }

        # Top-1å’ŒTop-5å‡†ç¡®ç‡
        top1_correct = sum(1 for r in successful_results if r['top1_correct'])
        top5_correct = sum(1 for r in successful_results if r['top5_correct'])

        top1_accuracy = top1_correct / success_count
        top5_accuracy = top5_correct / success_count

        # å»¶è¿Ÿç»Ÿè®¡
        avg_latency = statistics.mean(self.latencies)
        p50_latency = statistics.median(self.latencies)
        sorted_latencies = sorted(self.latencies)
        p95_latency = sorted_latencies[int(len(sorted_latencies) * 0.95)]
        p99_latency = sorted_latencies[int(len(sorted_latencies) * 0.99)]

        # æŒ‰ç±»åˆ«ç»Ÿè®¡
        class_stats = defaultdict(lambda: {'correct': 0, 'total': 0})
        for result in successful_results:
            gt = result['ground_truth']
            class_stats[gt]['total'] += 1
            if result['top1_correct']:
                class_stats[gt]['correct'] += 1

        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
        class_accuracies = {}
        for class_name, stats in class_stats.items():
            if stats['total'] > 0:
                class_accuracies[class_name] = stats['correct'] / stats['total']

        return {
            'total_images': total_count,
            'successful_tests': success_count,
            'failed_tests': total_count - success_count,
            'top1_accuracy': top1_accuracy,
            'top5_accuracy': top5_accuracy,
            'avg_latency_ms': avg_latency,
            'p50_latency_ms': p50_latency,
            'p95_latency_ms': p95_latency,
            'p99_latency_ms': p99_latency,
            'min_latency_ms': min(self.latencies),
            'max_latency_ms': max(self.latencies),
            'throughput_rps': success_count / (sum(self.latencies) / 1000),
            'class_accuracies': class_accuracies,
            'detailed_results': successful_results
        }

    def print_results(self, metrics: Dict):
        """æ‰“å°æµ‹è¯•ç»“æœ"""
        print(f"{'='*60}")
        print(f"ğŸ“Š æ¨¡å‹æ•ˆæœæµ‹è¯•ç»“æœ")
        print(f"{'='*60}")
        print(f"   æ€»å›¾ç‰‡æ•°: {metrics['total_images']}")
        print(f"   æˆåŠŸæµ‹è¯•: {metrics['successful_tests']}")
        print(f"   å¤±è´¥æµ‹è¯•: {metrics['failed_tests']}")
        print()
        print(f"   Top-1 å‡†ç¡®ç‡: {metrics['top1_accuracy']:.4f} ({metrics['top1_accuracy']*100:.2f}%)")
        print(f"   Top-5 å‡†ç¡®ç‡: {metrics['top5_accuracy']:.4f} ({metrics['top5_accuracy']*100:.2f}%)")
        print()
        print(f"   å¹³å‡å»¶è¿Ÿ: {metrics['avg_latency_ms']:.2f}ms")
        print(f"   P50 å»¶è¿Ÿ: {metrics['p50_latency_ms']:.2f}ms")
        print(f"   P95 å»¶è¿Ÿ: {metrics['p95_latency_ms']:.2f}ms")
        print(f"   P99 å»¶è¿Ÿ: {metrics['p99_latency_ms']:.2f}ms")
        print(f"   æœ€å°å»¶è¿Ÿ: {metrics['min_latency_ms']:.2f}ms")
        print(f"   æœ€å¤§å»¶è¿Ÿ: {metrics['max_latency_ms']:.2f}ms")
        print(f"   ååé‡: {metrics['throughput_rps']:.2f} RPS")
        print()

        # æ‰“å°å„ç±»åˆ«å‡†ç¡®ç‡
        if metrics.get('class_accuracies'):
            print(f"   å„ç±»åˆ«å‡†ç¡®ç‡:")
            sorted_classes = sorted(metrics['class_accuracies'].items(), key=lambda x: x[1], reverse=True)
            for class_name, accuracy in sorted_classes:
                print(f"      {class_name}: {accuracy:.4f} ({accuracy*100:.2f}%)")

    def save_results(self, metrics: Dict):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
        # åˆ›å»ºè¾“å‡ºå­—å…¸ï¼ˆå»é™¤è¯¦ç»†ç»“æœä»¥å‡å°‘æ–‡ä»¶å¤§å°ï¼‰
        output_metrics = metrics.copy()
        if 'detailed_results' in output_metrics:
            del output_metrics['detailed_results']

        with open(self.output_file, 'w') as f:
            json.dump({'metrics': output_metrics}, f, indent=2)

        print(f"   ğŸ“„ ç»“æœå·²ä¿å­˜åˆ°: {self.output_file}")


def main():
    parser = argparse.ArgumentParser(description='æ¨¡å‹æ•ˆæœæµ‹è¯•è„šæœ¬')
    parser.add_argument('--base-url', required=True, help='APIåŸºç¡€URL')
    parser.add_argument('--data-dir', required=True, help='æµ‹è¯•æ•°æ®ç›®å½•')
    parser.add_argument('--output', help='ç»“æœè¾“å‡ºæ–‡ä»¶ï¼ˆJSONæ ¼å¼ï¼‰')

    args = parser.parse_args()

    # åˆ›å»ºæµ‹è¯•å™¨
    tester = AccuracyTester(
        base_url=args.base_url,
        data_dir=args.data_dir,
        output_file=args.output
    )

    # è¿è¡Œæµ‹è¯•
    metrics = tester.run()

    return metrics


if __name__ == '__main__':
    main()
