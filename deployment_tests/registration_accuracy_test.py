#!/usr/bin/env python3
"""
Registration OCR å‡†ç¡®ç‡æµ‹è¯•è„šæœ¬
æµ‹è¯• OCR è¯†åˆ«çš„å‡†ç¡®ç‡å’Œç½®ä¿¡åº¦
"""

import argparse
import base64
import json
import time
from pathlib import Path
from typing import Dict, List, Tuple
import requests

# æ³¨å†Œå·æ ¼å¼éªŒè¯
REGISTRATION_PATTERN = r"^[A-Z]{1,2}-[A-HJ-NP-Z0-9]{1,5}$"


class RegistrationAccuracyTester:
    """æ³¨å†Œå· OCR å‡†ç¡®ç‡æµ‹è¯•å™¨"""

    def __init__(self, base_url: str, data_dir: str, output_file: str = None):
        self.base_url = base_url.rstrip('/')
        self.data_dir = Path(data_dir)
        self.output_file = output_file

        # ç»Ÿè®¡æ•°æ®
        self.results = []
        self.latencies = []

    def parse_ground_truth(self, filename: str) -> str:
        """ä»æ–‡ä»¶åè§£æçœŸå®æ³¨å†Œå·"""
        # å‡è®¾æ–‡ä»¶åæ ¼å¼ä¸º: REGISTRATION-0001.jpg
        # ä¾‹å¦‚: B-1234-0001.jpg
        parts = filename.replace('.jpg', '').split('-')
        if parts:
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ªåŒ¹é…æ³¨å†Œå·æ ¼å¼çš„éƒ¨åˆ†
            import re
            for part in parts:
                if re.match(REGISTRATION_PATTERN, part):
                    return part
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œè¿”å›ç¬¬ä¸€ä¸ªéƒ¨åˆ†ä½œä¸ºå¤‡ç”¨
            return parts[0]
        return filename

    def load_test_images(self) -> List[Tuple[Path, str]]:
        """åŠ è½½æµ‹è¯•å›¾ç‰‡å’ŒçœŸå®æ ‡ç­¾"""
        images = []
        for img_file in self.data_dir.glob('*.jpg'):
            ground_truth = self.parse_ground_truth(img_file.name)
            images.append((img_file, ground_truth))
        return images

    def test_single_image(self, image_path: Path, ground_truth: str) -> Dict:
        """æµ‹è¯•å•å¼ å›¾ç‰‡"""
        # åŠ è½½å›¾ç‰‡
        with open(image_path, 'rb') as f:
            img_data = f.read()
        base64_img = base64.b64encode(img_data).decode()

        # å‘é€APIè¯·æ±‚
        start_time = time.time()
        try:
            response = requests.post(
                f"{self.base_url}/api/v1/registration",
                json={'image': base64_img},
                timeout=120  # Qwen API å¯èƒ½è¾ƒæ…¢
            )
            duration = (time.time() - start_time) * 1000  # æ¯«ç§’

            if response.status_code == 200:
                result = response.json()
                registration = result['registration']
                confidence = result['confidence']
                raw_text = result.get('raw_text', '')

                # éªŒè¯æ³¨å†Œå·æ ¼å¼
                import re
                is_valid = bool(re.match(REGISTRATION_PATTERN, registration))

                return {
                    'success': True,
                    'ground_truth': ground_truth,
                    'predicted': registration,
                    'is_match': registration == ground_truth,
                    'confidence': confidence,
                    'raw_text': raw_text,
                    'is_valid_format': is_valid,
                    'latency_ms': duration,
                    'image': image_path.name
                }
            else:
                return {
                    'success': False,
                    'error': f"HTTP {response.status}",
                    'ground_truth': ground_truth,
                    'latency_ms': duration,
                    'image': image_path.name
                }

        except Exception as e:
            duration = (time.time() - start_time) * 1000
            return {
                'success': False,
                'error': str(e),
                'ground_truth': ground_truth,
                'latency_ms': duration,
                'image': image_path.name
            }

    def run(self):
        """è¿è¡Œå®Œæ•´çš„æµ‹è¯•"""
        print(f"ğŸ§ª å¼€å§‹æ³¨å†Œå· OCR å‡†ç¡®ç‡æµ‹è¯•...")
        print(f"   åŸºç¡€URL: {self.base_url}")
        print(f"   æ•°æ®ç›®å½•: {self.data_dir}")
        print()

        # åŠ è½½æµ‹è¯•å›¾ç‰‡
        test_images = self.load_test_images()
        print(f"   æ‰¾åˆ° {len(test_images)} å¼ æµ‹è¯•å›¾ç‰‡")
        print()

        # æµ‹è¯•æ¯å¼ å›¾ç‰‡
        for i, (image_path, ground_truth) in enumerate(test_images, 1):
            print(f"   æµ‹è¯• {i}/{len(test_images)}: {image_path.name}")
            result = self.test_single_image(image_path, ground_truth)
            self.results.append(result)

            if result['success']:
                self.latencies.append(result['latency_ms'])

                # æ˜¾ç¤ºç»“æœ
                status = "âœ“" if result['is_match'] else "âœ—"
                print(f"      {status} çœŸå®: {ground_truth}, é¢„æµ‹: {result['predicted']}, ç½®ä¿¡åº¦: {result['confidence']:.2f}")
            else:
                print(f"      âœ— é”™è¯¯: {result.get('error', 'Unknown')}")

        # è®¡ç®—æŒ‡æ ‡
        metrics = self.calculate_metrics()
        self.print_results(metrics)

        # ä¿å­˜ç»“æœ
        if self.output_file:
            self.save_results(metrics)

        return metrics

    def calculate_metrics(self) -> Dict:
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        successful_results = [r for r in self.results if r['success']]
        total_count = len(self.results)
        success_count = len(successful_results)

        if success_count == 0:
            return {
                'total_images': total_count,
                'successful_tests': success_count,
                'failed_tests': total_count - success_count,
                'accuracy': 0.0,
                'valid_format_rate': 0.0,
                'avg_confidence': 0.0,
                'avg_latency_ms': 0.0,
                'p50_latency_ms': 0.0,
                'p95_latency_ms': 0.0,
                'p99_latency_ms': 0.0
            }

        # å‡†ç¡®ç‡
        correct_predictions = sum(1 for r in successful_results if r['is_match'])
        accuracy = correct_predictions / success_count

        # æ ¼å¼æœ‰æ•ˆæ€§
        valid_format = sum(1 for r in successful_results if r['is_valid_format'])
        valid_format_rate = valid_format / success_count

        # å¹³å‡ç½®ä¿¡åº¦
        avg_confidence = sum(r['confidence'] for r in successful_results) / success_count

        # å»¶è¿Ÿç»Ÿè®¡
        avg_latency = sum(self.latencies) / len(self.latencies)
        sorted_latencies = sorted(self.latencies)
        p50_latency = sorted_latencies[len(sorted_latencies) // 2]
        p95_latency = sorted_latencies[int(len(sorted_latencies) * 0.95)]
        p99_latency = sorted_latencies[int(len(sorted_latencies) * 0.99)]

        # ç½®ä¿¡åº¦åˆ†å¸ƒ
        high_conf_count = sum(1 for r in successful_results if r['confidence'] >= 0.8)
        high_conf_rate = high_conf_count / success_count

        return {
            'total_images': total_count,
            'successful_tests': success_count,
            'failed_tests': total_count - success_count,
            'accuracy': accuracy,
            'valid_format_rate': valid_format_rate,
            'avg_confidence': avg_confidence,
            'high_confidence_rate': high_conf_rate,
            'avg_latency_ms': avg_latency,
            'p50_latency_ms': p50_latency,
            'p95_latency_ms': p95_latency,
            'p99_latency_ms': p99_latency,
            'min_latency_ms': min(self.latencies),
            'max_latency_ms': max(self.latencies),
            'throughput_rps': success_count / (sum(self.latencies) / 1000),
            'detailed_results': successful_results
        }

    def print_results(self, metrics: Dict):
        """æ‰“å°æµ‹è¯•ç»“æœ"""
        print(f"{'='*60}")
        print(f"ğŸ“Š æ³¨å†Œå· OCR å‡†ç¡®ç‡æµ‹è¯•ç»“æœ")
        print(f"{'='*60}")
        print(f"   æ€»å›¾ç‰‡æ•°: {metrics['total_images']}")
        print(f"   æˆåŠŸæµ‹è¯•: {metrics['successful_tests']}")
        print(f"   å¤±è´¥æµ‹è¯•: {metrics['failed_tests']}")
        print()
        print(f"   è¯†åˆ«å‡†ç¡®ç‡: {metrics['accuracy']:.4f} ({metrics['accuracy']*100:.2f}%)")
        print(f"   æ ¼å¼æœ‰æ•ˆæ€§: {metrics['valid_format_rate']:.4f} ({metrics['valid_format_rate']*100:.2f}%)")
        print(f"   å¹³å‡ç½®ä¿¡åº¦: {metrics['avg_confidence']:.4f}")
        print(f"   é«˜ç½®ä¿¡åº¦æ¯”ä¾‹: {metrics['high_confidence_rate']:.4f} ({metrics['high_confidence_rate']*100:.2f}%)")
        print()
        print(f"   å¹³å‡å»¶è¿Ÿ: {metrics['avg_latency_ms']:.2f}ms")
        print(f"   P50 å»¶è¿Ÿ: {metrics['p50_latency_ms']:.2f}ms")
        print(f"   P95 å»¶è¿Ÿ: {metrics['p95_latency_ms']:.2f}ms")
        print(f"   P99 å»¶è¿Ÿ: {metrics['p99_latency_ms']:.2f}ms")
        print(f"   æœ€å°å»¶è¿Ÿ: {metrics['min_latency_ms']:.2f}ms")
        print(f"   æœ€å¤§å»¶è¿Ÿ: {metrics['max_latency_ms']:.2f}ms")
        print(f"   ååé‡: {metrics['throughput_rps']:.2f} RPS")
        print()

    def save_results(self, metrics: Dict):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
        # åˆ›å»ºè¾“å‡ºå­—å…¸ï¼ˆå»é™¤è¯¦ç»†ç»“æœä»¥å‡å°‘æ–‡ä»¶å¤§å°ï¼‰
        output_metrics = metrics.copy()
        if 'detailed_results' in output_metrics:
            del output_metrics['detailed_results']

        with open(self.output_file, 'w') as f:
            json.dump({'metrics': output_metrics, 'results': self.results}, f, indent=2)

        print(f"   ğŸ“„ ç»“æœå·²ä¿å­˜åˆ°: {self.output_file}")


def main():
    parser = argparse.ArgumentParser(description='æ³¨å†Œå· OCR å‡†ç¡®ç‡æµ‹è¯•è„šæœ¬')
    parser.add_argument('--base-url', required=True, help='APIåŸºç¡€URL')
    parser.add_argument('--data-dir', required=True, help='æµ‹è¯•æ•°æ®ç›®å½•')
    parser.add_argument('--output', help='ç»“æœè¾“å‡ºæ–‡ä»¶ï¼ˆJSONæ ¼å¼ï¼‰')

    args = parser.parse_args()

    # åˆ›å»ºæµ‹è¯•å™¨
    tester = RegistrationAccuracyTester(
        base_url=args.base_url,
        data_dir=args.data_dir,
        output_file=args.output
    )

    # è¿è¡Œæµ‹è¯•
    metrics = tester.run()

    return metrics


if __name__ == '__main__':
    main()
